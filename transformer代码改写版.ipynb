{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成掩码张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    ones = np.ones((1, size, size))\n",
    "    triu = np.triu(ones, k=1).astype('uint8')\n",
    "    mask = 1 - triu\n",
    "    mask = torch.from_numpy(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路:  生成一个numpy数字 什么样的形状很重要,用np.triu()完成1,0数组, 然后1-这个数组 最后torch.from_numpy()转换成张量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: q点乘k的转置, 除以根号d_k  这一步可以称作1 scale dot_product\n",
    "2 进行掩码\n",
    "3 softmax\n",
    "4 dropout\n",
    "5 跟v 做点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2), -1) / math.sqrt(q.size(-1))\n",
    "    if mask is not None:\n",
    "        scaled = scaled.filled_mask(mask==0, -1e9)\n",
    "    attn = torch.functional.softmax(scaled, -1)\n",
    "    if droupout is not None:\n",
    "        attn = dropout(attn)\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: 1q,k,v 每个进行一个全连接层, 2  转换维度, 形状 3 attention 4 转换维度形状 5 全连接 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, n):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, head, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.linears = clones(torch.nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "        assert embedding_dim % head == 0\n",
    "        self.head = head\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.d_k = self.embedding_dim // self.head\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.attn = None\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q_view, k_view, v_view = [model(x).view(q.size(0), -1, self.head, self.d_k).transpose(1, 2) for model,x in zip(self.linears, (q, k, v))]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        x, self.attn = attention(q_view, k_view, v_view, mask=mask, dropout=self.dropout1)\n",
    "        x = x.transpose(1, 2).congurous().view(q.size(0), -1, self.embedding_dim)\n",
    "        x = self.linears[-1](x)\n",
    "        return self.norm(q + self.dropout2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clones(torch.nn.Linear(5, 5), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: 全连接 > relu> dropout> 全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linears = clones(torch.nn.Linear(d_model, d_ff), 2)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linears[1](self.dropout(torch.functional.relu(self.linears[0](x))))\n",
    "        return self.norm(x + self.dropout2(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: 减均值 除标准差+极小值防止分母为零 加参数张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, epsilon=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(embedding_dim))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(embedding_dim))\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x = self.gamma * (x - mean) / (std + self.epsilon)  + self.beta\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = LayerNorm(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 子层链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路 sublayer > dropout > +x > normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SublayerConnection(torch.nn.Module):\n",
    "#     def __init(self, embedding_dim):\n",
    "#         super(SublayerConnection, self).__init__()\n",
    "#         self.normalize = LayerNorm(embedding_dim)\n",
    "#     def forward(x, )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: multiattention > feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, self_attn, ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.ff = ff\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attention = self.self_attn(x, x, x, mask)\n",
    "        return self.ff(attention)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_layer, n):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(encoder_layer, n)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路:  self_attention >  attention > feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, attention_layer, ff_layer):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attn_layers =  clones(attention_layer, 2)\n",
    "        self.ff = ff_layer\n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        self_attn = self.attn_layers[0](x, x, x, target_mask)\n",
    "        x = self.attn_layers[1](self_attn, encoder_output, encoder_output, source_mask)\n",
    "        x = self.ff(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, DecoderLayer, n):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = clones(DecoderLayer, n)\n",
    "        \n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, source_mask, target_mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: 全连接 > log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear = torch.nn.Linear(embedding_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = torch.functional.log_softmax(self.linear(x), dim=-1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoderdecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路: source, source_mask > encoder > encoder_output,    target, encoder_output, source_mask, target_mask>decoder > decoder_output>generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embedding, target_embedding, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embedding = source_embedding\n",
    "        self.target_embedding = target_embedding\n",
    "        self.generator = generator\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        encoder_output = self.encoder(self.source_embedding(x), source_mask)\n",
    "        decoder_output = self.decoder(self.target_embedding(target), encoder_output, source_mask, target_mask)\n",
    "        return self.generator(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(source_vocab, target_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(d_model, head)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(Encoder(EncoderLayer(c(attn), c(ff)), N),\n",
    "        Decoder(DecoderLayer(c(attn), c(ff)), N),\n",
    "        torch.nn.Sequential(Embeddings(d_model, source_vocab), c(position)),\n",
    "        torch.nn.Sequential(Embeddings(d_model, target_vocab), c(position)),\n",
    "        Generator(d_model, target_vocab))\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            torch.nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小.\"\"\"\n",
    "        # 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.\n",
    "        super(Embeddings, self).__init__()\n",
    "        # 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut\n",
    "        self.lut = torch.nn.Embedding(vocab, d_model)\n",
    "        # 最后就是将d_model传入类中\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"可以将其理解为该层的前向传播逻辑，所有层中都会有此函数\n",
    "           当传给该类的实例化对象参数时, 自动调用该类函数\n",
    "           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量\"\"\"\n",
    "\n",
    "        # 将x传给self.lut并与根号下self.d_model相乘作为结果返回\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度, \n",
    "           dropout: 置0比率, max_len: 每个句子的最大长度\"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "     \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "       \n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，\n",
    "        # 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. \n",
    "        # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward函数的参数是x, 表示文本序列的词嵌入表示\"\"\"\n",
    "       \n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        # 最后使用self.dropout对象进行'丢弃'操作, 并返回结果.\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-c66956fd4a0a>:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (source_embedding): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(11, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (target_embedding): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(11, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (linear): Linear(in_features=512, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model(11, 11)\n",
    "# model = MultiHeadedAttention(embedding_dim=512, head=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
